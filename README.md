# Artificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons

**Authors:**  
Rahul Gorijavolu¬π¬≤‚Å∂, Ife Shoyombo¬π¬≤, Saurav Boparai¬≤¬≥, Andrew S. Cho¬π¬≤, Emily O‚ÄôConnell¬π,  
Kaushik Madapati¬≤‚Å¥‚Å∂, Vinayak Mathur‚Å∂, Megan K. Applewhite‚Å∑, Anne M. Stey‚Åµ, Leo A. Celi‚Å∂  

**Affiliations:**  
¬π Johns Hopkins University School of Medicine  
¬≤ ARGOS (AI for Responsible, Generalizable, and Open Surgical Research Group)  
¬≥ Hackensack Meridian Health School of Medicine  
‚Å¥ UC Berkeley College of Engineering  
‚Åµ Northwestern University Department of Surgery  
‚Å∂ MIT Laboratory for Computational Physiology  
‚Å∑ University of Chicago Department of Surgery  

---

## üß© Overview

This repository supports the study *‚ÄúArtificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons.‚Äù*  
The work investigates how leading text-to-image diffusion systems represent professional identities‚Äîspecifically surgeons‚Äîunder neutral prompting, and compares outputs to real-world workforce demographics.

The study audits three major models:
- **DALL¬∑E 3** (OpenAI)
- **ImageFX** (Google)
- **FLUX.1-dev** (OpenArt)

We identify significant disparities in demographic representation, operating context, and visual realism, highlighting how alignment and bias mitigation strategies can both improve inclusivity and introduce new distortions.

---

## üî¨ Research Questions

1. How do text-to-image models depict surgeons when given a neutral prompt (‚Äúsurgeon‚Äù)?
2. Do generated demographics (gender, race) align with real U.S. surgical workforce statistics?
3. How do alignment or prompt-modification behaviors (e.g., in DALL¬∑E 3) influence representational outcomes?
4. What trade-offs exist between fairness, realism, and authenticity in generative medical imagery?

---

## ‚öôÔ∏è Methods Summary

- **Design:** Cross-sectional evaluation  
- **Prompt:** `"surgeon"` (neutral, default settings)  
- **Models Evaluated:** DALL¬∑E 3, ImageFX, OpenArt FLUX.1-dev  
- **Sample Size:** 100 images per model (n = 300 total)  
- **Annotation:**  
  - Attributes: Gender, race/ethnicity, hair/eye color, operating context, deformities  
  - Annotators: 3 blinded medical doctoral students  
- **Statistical Analysis:**  
  - Chi-square, Kruskal-Wallis, and Fleiss‚Äô Kappa (RStudio v4.2.1)  
  - Follow-up DALL¬∑E 3 audit: 250 API calls, prompt parsing (Python v3.11.1)

---

## üìä Results Summary

- **DALL¬∑E 3**: Overrepresented *female* (83%) and *people-of-color* (67%) surgeons  
- **ImageFX / FLUX.1-dev**: Overrepresented *male* and *White* surgeons  
- **Deformities:** Highest in DALL¬∑E 3 (55%)  
- **Prompt Audit:** 75.8% of DALL¬∑E 3‚Äôs internal prompts added gender/race modifiers, often favoring ‚Äúfemale Asian surgeon‚Äù depictions  

While alignment strategies improved diversity, they also introduced higher artifact rates‚Äîhighlighting tensions between *inclusivity* and *fidelity*.

---

## üß∞ Repository Contents

```
üìÅ Artificial-Identities/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ generated_images/              # Sample or reference outputs (if available)
‚îÇ   ‚îú‚îÄ‚îÄ annotations.csv                # Annotator results
‚îÇ   ‚îî‚îÄ‚îÄ prompt_audit_dalle3.csv        # Revised prompt logs
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.R
‚îÇ   ‚îú‚îÄ‚îÄ interrater_reliability.R
‚îÇ   ‚îú‚îÄ‚îÄ statistical_analysis.R
‚îÇ   ‚îî‚îÄ‚îÄ dalle3_prompt_audit.py
‚îÇ
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ demographics_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ deformity_rates.png
‚îÇ   ‚îî‚îÄ‚îÄ intersectionality_heatmap.png
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ summary_statistics.csv
‚îÇ   ‚îú‚îÄ‚îÄ chi_square_results.csv
‚îÇ   ‚îî‚îÄ‚îÄ kappa_scores.csv
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üí° Key Findings

- Text-to-image models differ drastically in demographic representation, even under identical prompts.  
- DALL¬∑E 3 actively modifies neutral prompts, introducing race and gender descriptors.  
- ‚ÄúFairness‚Äù adjustments can improve diversity metrics but may degrade image quality.  
- Systematic auditing and transparency (e.g., model cards, labeling) are essential for ethical deployment in healthcare and education.

---

## üìà Citation

If you use this repository, please cite:

> Gorijavolu R, Shoyombo I, Boparai S, Cho A, O‚ÄôConnell E, Madapati K, Mathur V, Applewhite M, Stey A, Celi L.  
> *Artificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons.*  
> Johns Hopkins University / MIT Laboratory for Computational Physiology, 2025.

---

## üìú Ethics and Data Use

This project used only AI-generated imagery and publicly available data.  
No human or patient data were involved, and IRB approval was not required.

---

## ü§ù Acknowledgments

This research was supported by:
- **Johns Hopkins Institute for Clinical and Translational Research (ICTR)**  
- **MIT Laboratory for Computational Physiology**  
- **NIH NCATS (T32TR004928)**  
- **Bridge2AI (OT2OD032701)**  

We thank **Anne Stey**, **Megan Applewhite**, and **Leo Celi** for mentorship, and the ARGOS team for contributions to annotation and statistical validation.

---

## üîó Links

- **Publication:** *(in preparation / under submission)*  
- **Code Repository:** [ARGOS-Lab/Bias-in-Diffusion-Model-Generated-Images-of-Surgeons](https://github.com/ARGOS-Lab/Bias-in-Diffusion-Model-Generated-Images-of-Surgeons)
- **ARGOS Research Group:** [https://argos-lab.org](https://argos-lab.org)

---

## üßæ License

This project is licensed under the MIT License ‚Äî see the `LICENSE` file for details.
