# Artificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons

**Authors:**  
Rahul Gorijavolu¬π¬≤‚Å∂, Ife Shoyombo¬π¬≤, Saurav Boparai¬≤¬≥, Andrew S. Cho¬π¬≤, Emily O‚ÄôConnell¬π,  
Kaushik Madapati¬≤‚Å¥‚Å∂, Vinayak Mathur‚Å∂, Megan K. Applewhite‚Å∑, Anne M. Stey‚Åµ, Leo A. Celi‚Å∂  

**Affiliations:**  
¬π Johns Hopkins University School of Medicine  
¬≤ ARGOS (AI for Responsible, Generalizable, and Open Surgical Research Group)  
¬≥ Hackensack Meridian Health School of Medicine  
‚Å¥ UC Berkeley College of Engineering  
‚Åµ Northwestern University Department of Surgery  
‚Å∂ MIT Laboratory for Computational Physiology  
‚Å∑ University of Chicago Department of Surgery  

---

## üß© Overview

This repository supports the study *‚ÄúArtificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons.‚Äù*  
The project examines how leading text-to-image diffusion models portray surgeons when prompted with the neutral term **‚Äúsurgeon‚Äù**, assessing demographic and contextual bias relative to U.S. surgical workforce benchmarks.

Models analyzed:
- **DALL¬∑E 3** (OpenAI)
- **ImageFX** (Google)
- **FLUX.1-dev** (OpenArt)

---

## üìÇ Repository Structure

```
üìÅ Artificial-Identities/
‚îÇ
‚îú‚îÄ‚îÄ data analysis/
‚îÇ   ‚îú‚îÄ‚îÄ chi_square_trend.py           # Chi-square test and trend analysis
‚îÇ   ‚îî‚îÄ‚îÄ significance.py               # Significance testing and visualization helpers
‚îÇ
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ dalle_results_figure.png      # Main visualization of demographic results
‚îÇ   ‚îî‚îÄ‚îÄ dalle_results_table.pdf       # Table summarizing comparative outputs
‚îÇ
‚îú‚îÄ‚îÄ images/                           # All generated surgeon images (DALL¬∑E 3, ImageFX, FLUX.1-dev)
‚îÇ
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ chi_square_trends_results_r.csv
‚îÇ   ‚îú‚îÄ‚îÄ dalle_results.csv
‚îÇ   ‚îî‚îÄ‚îÄ data_final.csv                # Cleaned dataset for statistical analysis
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ dalle.py                      # DALL¬∑E 3 generation and prompt audit script
‚îÇ   ‚îî‚îÄ‚îÄ figures.py                    # Figure plotting and data visualization
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## üî¨ Study Summary

- **Design:** Cross-sectional image audit  
- **Prompt:** `"surgeon"` (neutral input, default parameters)  
- **Models:** DALL¬∑E 3, ImageFX, FLUX.1-dev  
- **Sample:** 100 images per model (n = 300 total)  
- **Annotations:**  
  - Gender, race/ethnicity, contextual cues (operating room, deformities, etc.)  
  - Three blinded raters (doctoral medical students)  
- **Analysis Tools:**  
  - RStudio (v4.2.1) for statistical evaluation  
  - Python (v3.11.1) for DALL¬∑E 3 prompt audit and intersectionality matrix  

---

## üìä Highlights

| Model | Female Surgeons | Persons of Color | Deformity Rate |
|--------|-----------------|------------------|----------------|
| **DALL¬∑E 3** | 83% | 67% | 55% |
| **ImageFX** | 4% | 4% | 23% |
| **FLUX.1-dev** | 0% | 0% | 22% |

> DALL¬∑E 3‚Äôs API automatically inserted demographic modifiers into neutral prompts (e.g., ‚Äúfemale Asian surgeon‚Äù), revealing hidden alignment steering that affected both diversity and image fidelity.

---

## üí° Key Insights

- Diffusion models differ dramatically in demographic portrayal under identical prompts.  
- Alignment mechanisms can overcorrect, improving diversity but introducing realism artifacts.  
- Transparent prompt reporting, fairness auditing, and labeling are essential for responsible use in healthcare imagery.  

---

## üìà Citation

If you use this repository, please cite:

> Gorijavolu R, Shoyombo I, Boparai S, Cho A, O‚ÄôConnell E, Madapati K, Mathur V, Applewhite M, Stey A, Celi L.  
> *Artificial Identities: Analyzing Bias in Diffusion Model-Generated Images of Surgeons.*  
> Johns Hopkins University / MIT Laboratory for Computational Physiology, 2025.

---

## ‚öñÔ∏è Ethics and Data Use

This study used only AI-generated imagery and publicly available datasets.  
No human subjects or patient data were involved; IRB approval was not required.

---

## ü§ù Acknowledgments

Supported by:
- **Johns Hopkins ICTR** (T32TR004928)  
- **MIT Laboratory for Computational Physiology**  
- **Bridge2AI (OT2OD032701)**  

With special thanks to mentors **Anne Stey**, **Megan Applewhite**, and **Leo Celi**, and the ARGOS research group.

---

## üîó Links

- Repository: [ARGOS-Lab/Bias-in-Diffusion-Model-Generated-Images-of-Surgeons](https://github.com/ARGOS-Lab/Bias-in-Diffusion-Model-Generated-Images-of-Surgeons)

---

## üßæ License

This project is licensed under the MIT License.
